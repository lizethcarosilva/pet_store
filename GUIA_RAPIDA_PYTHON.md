# ğŸš€ GUÃA RÃPIDA - PROYECTO PYTHON PARA ANÃLISIS PET STORE

## ğŸ“‹ Contenido del Proyecto

Este paquete incluye:

1. **`DATABASE_DOCUMENTATION_FOR_PYTHON.md`** - DocumentaciÃ³n completa de la base de datos
2. **`python_integration_example.py`** - Script de ejemplo con anÃ¡lisis y funciones
3. **`requirements.txt`** - Dependencias necesarias
4. **`config_template.env`** - Plantilla de configuraciÃ³n
5. **Esta guÃ­a** - Instrucciones paso a paso

---

## âš¡ INICIO RÃPIDO (5 minutos)

### **Paso 1: Instalar Python**

AsegÃºrate de tener Python 3.9 o superior:

```bash
python --version
```

### **Paso 2: Crear entorno virtual**

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### **Paso 3: Instalar dependencias**

```bash
pip install -r requirements.txt
```

**Nota:** La instalaciÃ³n puede tardar 5-10 minutos (especialmente TensorFlow).

### **Paso 4: Configurar variables de entorno**

Copia `config_template.env` como `.env`:

```bash
# Windows
copy config_template.env .env

# Linux/Mac
cp config_template.env .env
```

### **Paso 5: Ejecutar el anÃ¡lisis**

```bash
python python_integration_example.py
```

**Â¡Listo!** El script generarÃ¡:
- 5 grÃ¡ficos en formato PNG
- 1 archivo CSV con dataset para Machine Learning
- AnÃ¡lisis impresos en consola

---

## ğŸ“Š LO QUE HACE EL SCRIPT DE EJEMPLO

### **1. AnÃ¡lisis de Servicios MÃ¡s Utilizados**
```python
df_servicios = obtener_servicios_mas_usados(conn)
```
**Resultado:**
- Tabla con ranking de servicios
- GrÃ¡fico de barras: `servicios_mas_usados.png`

### **2. Tipos de Mascota MÃ¡s Atendidas**
```python
df_mascotas = obtener_mascotas_por_servicio(conn)
```
**Resultado:**
- DistribuciÃ³n por tipo (perro, gato, ave, etc.)
- GrÃ¡ficos: `mascotas_por_servicio.png`

### **3. DÃ­as y Horas con MÃ¡s AtenciÃ³n**
```python
df_dias = obtener_dias_con_mas_atencion(conn)
df_horas = obtener_horas_con_mas_atencion(conn)
```
**Resultado:**
- PatrÃ³n de demanda por dÃ­a de la semana
- PatrÃ³n de demanda por hora
- GrÃ¡ficos: `dias_con_mas_atencion.png`, `horas_con_mas_atencion.png`

### **4. AnÃ¡lisis de Ingresos**
```python
df_ingresos = obtener_ingresos_por_servicio(conn)
```
**Resultado:**
- Rentabilidad por servicio
- GrÃ¡fico: `ingresos_por_servicio.png`

### **5. Dataset para Machine Learning**
```python
df_ml = exportar_dataset_ml(conn)
```
**Resultado:**
- Archivo CSV: `dataset_citas_ml.csv`
- Incluye features temporales, caracterÃ­sticas de mascotas, servicios, etc.

---

## ğŸ¤– FUNCIONES PARA CHATBOT

El script incluye funciones listas para integrar en un chatbot:

### **Consultar historial de una mascota**
```python
historial = obtener_historial_mascota(conn, pet_id=1)
```

### **Ver vacunas pendientes**
```python
vacunas = obtener_vacunas_pendientes(conn, pet_id=1)
```

### **PrÃ³ximas citas de un cliente**
```python
citas = obtener_proximas_citas(conn, client_id=1)
```

---

## ğŸ§  MODELO DE RED NEURONAL - EJEMPLO BÃSICO

### **Paso 1: Cargar el dataset**

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow import keras

# Cargar dataset
df = pd.read_csv('dataset_citas_ml.csv')

# Seleccionar features
features = ['dia_semana', 'hora', 'mes', 'service_id', 'edad_mascota']
X = df[features].fillna(0)

# Target: Predecir si la cita serÃ¡ completada
y = df['asistio']

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### **Paso 2: Crear modelo de red neuronal**

```python
# Definir arquitectura
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(1, activation='sigmoid')  # ClasificaciÃ³n binaria
])

# Compilar
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy', 'AUC']
)

# Entrenar
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# Evaluar
test_loss, test_acc, test_auc = model.evaluate(X_test, y_test)
print(f"PrecisiÃ³n en test: {test_acc:.2%}")
```

### **Paso 3: Hacer predicciones**

```python
# Predecir para una nueva cita
nueva_cita = [[3, 10, 11, 5, 2]]  # MiÃ©rcoles, 10 AM, Noviembre, Servicio 5, Mascota 2 aÃ±os
probabilidad = model.predict(nueva_cita)
print(f"Probabilidad de asistir: {probabilidad[0][0]:.2%}")
```

---

## ğŸ“ˆ CREAR DASHBOARD CON STREAMLIT

### **Paso 1: Crear archivo `dashboard.py`**

```python
import streamlit as st
import pandas as pd
import psycopg2
import plotly.express as px

# ConfiguraciÃ³n de la pÃ¡gina
st.set_page_config(page_title="Pet Store Analytics", layout="wide")

# TÃ­tulo
st.title("ğŸ¾ Pet Store - Dashboard de AnÃ¡lisis")

# Conectar a la base de datos
@st.cache_resource
def conectar_db():
    return psycopg2.connect(
        host='gondola.proxy.rlwy.net',
        port=22967,
        database='railway',
        user='postgres',
        password='LpEGFItXIhiOLcvpeWczptlFPxYnxhhI'
    )

conn = conectar_db()

# Cargar datos
@st.cache_data
def cargar_servicios():
    query = """
    SELECT s.nombre, COUNT(a.appointment_id) as total_citas
    FROM appointment a
    JOIN service s ON a.service_id = s.service_id
    WHERE a.activo = true
    GROUP BY s.nombre
    ORDER BY total_citas DESC;
    """
    return pd.read_sql(query, conn)

df_servicios = cargar_servicios()

# VisualizaciÃ³n
st.subheader("Servicios MÃ¡s Utilizados")
fig = px.bar(df_servicios, x='total_citas', y='nombre', orientation='h')
st.plotly_chart(fig, use_container_width=True)

# MÃ©tricas
col1, col2, col3 = st.columns(3)
col1.metric("Total Servicios", len(df_servicios))
col2.metric("Citas Totales", df_servicios['total_citas'].sum())
col3.metric("Promedio por Servicio", f"{df_servicios['total_citas'].mean():.1f}")
```

### **Paso 2: Ejecutar el dashboard**

```bash
streamlit run dashboard.py
```

Se abrirÃ¡ automÃ¡ticamente en `http://localhost:8501`

---

## ğŸ—‚ï¸ ESTRUCTURA DE PROYECTO SUGERIDA

```
mi-proyecto-petstore/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Datos crudos
â”‚   â”œâ”€â”€ processed/              # Datos procesados
â”‚   â””â”€â”€ models/                 # Modelos entrenados
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploracion.ipynb    # AnÃ¡lisis exploratorio
â”‚   â”œâ”€â”€ 02_modelo_prediccion.ipynb
â”‚   â””â”€â”€ 03_evaluacion.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ connection.py       # Funciones de conexiÃ³n
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ prediccion.py       # Modelo de predicciÃ³n
â”‚   â”‚   â””â”€â”€ clasificacion.py
â”‚   â”‚
â”‚   â”œâ”€â”€ chatbot/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ bot.py
â”‚   â”‚
â”‚   â””â”€â”€ dashboard/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ app.py
â”‚
â”œâ”€â”€ python_integration_example.py  # Script de ejemplo
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config_template.env
â”œâ”€â”€ DATABASE_DOCUMENTATION_FOR_PYTHON.md
â””â”€â”€ GUIA_RAPIDA_PYTHON.md
```

---

## ğŸ¯ CASOS DE USO ESPECÃFICOS

### **1. Â¿QuÃ© servicio es mÃ¡s utilizado?**

**Consulta SQL:**
```sql
SELECT s.nombre, COUNT(*) as total
FROM appointment a
JOIN service s ON a.service_id = s.service_id
GROUP BY s.nombre
ORDER BY total DESC;
```

**Python:**
```python
df = pd.read_sql(query, conn)
print(df.head(10))
```

---

### **2. Â¿QuÃ© tipo de mascota va mÃ¡s a servicios?**

**Consulta SQL:**
```sql
SELECT p.tipo, COUNT(a.appointment_id) as total_citas
FROM appointment a
JOIN pet p ON a.pet_id = p.pet_id
GROUP BY p.tipo
ORDER BY total_citas DESC;
```

---

### **3. Â¿QuÃ© dÃ­a hay mÃ¡s atenciÃ³n?**

**Consulta SQL:**
```sql
SELECT 
    CASE EXTRACT(DOW FROM fecha_hora)
        WHEN 0 THEN 'Domingo'
        WHEN 1 THEN 'Lunes'
        -- ...
    END as dia,
    COUNT(*) as total
FROM appointment
GROUP BY EXTRACT(DOW FROM fecha_hora)
ORDER BY total DESC;
```

---

## ğŸ” TIPS Y MEJORES PRÃCTICAS

### **1. Optimizar consultas largas**

```python
import time

inicio = time.time()
df = pd.read_sql(query, conn)
fin = time.time()
print(f"â±ï¸ Consulta ejecutada en {fin-inicio:.2f} segundos")
```

### **2. CachÃ© de datos en Streamlit**

```python
@st.cache_data(ttl=600)  # Cache por 10 minutos
def cargar_datos():
    return pd.read_sql(query, conn)
```

### **3. Manejo de errores**

```python
try:
    df = pd.read_sql(query, conn)
except Exception as e:
    print(f"âŒ Error: {e}")
    df = pd.DataFrame()  # DataFrame vacÃ­o
```

### **4. Exportar resultados**

```python
# CSV
df.to_csv('resultados.csv', index=False)

# Excel
df.to_excel('resultados.xlsx', index=False)

# JSON
df.to_json('resultados.json', orient='records')
```

---

## ğŸ“š RECURSOS ADICIONALES

### **Tutoriales recomendados:**

1. **Pandas:** https://pandas.pydata.org/docs/getting_started/intro_tutorials/
2. **TensorFlow:** https://www.tensorflow.org/tutorials
3. **Streamlit:** https://docs.streamlit.io/
4. **Machine Learning:** https://scikit-learn.org/stable/tutorial/

### **Datasets de ejemplo:**

- Kaggle: https://www.kaggle.com/datasets
- UCI ML Repository: https://archive.ics.uci.edu/ml/

---

## â“ SOLUCIÃ“N DE PROBLEMAS

### **Error: "No module named 'psycopg2'"**
```bash
pip install psycopg2-binary
```

### **Error: "Could not connect to database"**
- Verifica que las credenciales en `.env` sean correctas
- Revisa que tienes conexiÃ³n a internet
- Prueba con: `ping gondola.proxy.rlwy.net`

### **Error: "TensorFlow not found"**
```bash
# Instalar versiÃ³n CPU (mÃ¡s ligera)
pip install tensorflow-cpu==2.15.0
```

### **Dataset vacÃ­o**
- Verifica que hay datos en la base de datos
- Revisa la clÃ¡usula WHERE en tus queries
- Confirma que el `tenant_id` sea correcto

---

## ğŸ“ SIGUIENTE PASO

1. âœ… Ejecuta `python python_integration_example.py`
2. âœ… Revisa los grÃ¡ficos generados
3. âœ… Abre el archivo `dataset_citas_ml.csv` en Excel o Jupyter
4. âœ… Experimenta modificando las consultas SQL
5. âœ… Crea tu primer modelo de red neuronal
6. âœ… Implementa tu dashboard con Streamlit

---

**Â¡Ã‰xito con tu proyecto! ğŸš€**

Si tienes dudas o necesitas ayuda especÃ­fica, consulta la documentaciÃ³n completa en `DATABASE_DOCUMENTATION_FOR_PYTHON.md`

